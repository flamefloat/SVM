# 支持向量机（SVM）
SVM是一种二分类模型，它的基本模型是定义在特征空间上的线性分类器，间隔最大化使其有别于感知机；通过核技巧可成为非线性分类器。支持向量机的学习算法是求解凸二次规划的最优解法。通过核函数学习非线性支持向量机，等价于隐式地在高维特征空空间学习线性支持向量机。
## SVM的学习算法
样本点 $\left ( x_{i},y_{i} \right )$ 被超平面 $\left ( w,b \right )$ 正确分类时，点 $x_{i}$ 到超平面的几何间隔为
$$\gamma _{i}=y_{i}\left ( \frac{w}{\left \| w \right \|}\cdot x_{i}+\frac{b}{\left \| w \right \|} \right )$$
函数间隔与几何间隔关系为
$$\gamma =\frac{\widehat{\gamma }}{\left \| w \right \|}$$
超平面参数 $\left ( w,b \right )$ 成比例变化，函数间隔也成比例变化，但几何间隔不变。

SVM问题为求解几何间隔最大化的超平面，即：
$$\underset{w,b}{max}\: \gamma $$
$$s.t.\; y_{i}\left ( \frac{w}{\left \| w \right \|}\cdot x_{i}+\frac{b}{\left \| w \right \|} \right )\geq \gamma ,\; i=1,2,...,N$$
等价于
$$\underset{w,b}{max}\: \frac{\widehat{\gamma }}{\left \| w \right \|} $$
$$s.t.\; y_{i}\left ( w\cdot x_{i}+b\right )\geq \widehat{\gamma } ,\; i=1,2,...,N$$
令 $\widehat{\gamma }=1$ 原问题等价与
$$\underset{w,b}{min}\; \frac{1}{2}\left \| w \right \|^{2}$$
$$y_{i}\left ( w\cdot x_{i}+b\right )-1\geq 0 ,\; i=1,2,...,N$$
**凸优化问题**
>$$\underset{w}{min}\; f(w)$$
>$$s.t.\; g_{i}(w)\leq 0,i=1,2,...,k$$
>$$h_{i}(w)= 0,i=1,2,...,l$$
>其中目标函数 $f(w)$ 和约束函数 $g_{i}(w)$ 都是 $R^{n}$ 上的连续可微的凸函数，约束函数 $h_{i}(w)$ 是 $R^{n}$ 上的的仿射函数。

求解问题得到超平面 $w\cdot x+b=0$，分类决策函数为：
$$f(x)=sign(w\cdot x+b)$$
## SVM学习的对偶算法
引入拉格朗日乘子 $a_{i}\geq 0,i=1,2,...,N$，定义拉格朗日函数：
$$L(w,b,a)=\frac{1}{2}\left \| w \right \|^{2}-\sum_{i=1}^{N}a_{i}y_{i}\left ( w\cdot x_{i}+b\right )+\sum_{i=1}^{N}a_{i}$$
原始问题的对偶问题是极大极小问题：
$$\underset{a}{max}\underset{w,b}{min}L(w,b,a)$$
将函数 $L(w,b,a)$ 分别对 $(w,b)$ 求偏导，并令其等于0，得：
$$w=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$
$$\sum_{i=1}^{N}a_{i}y_{i}=0$$
于是得到对偶优化问题：
$$\underset{a}{min}\; \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}a_{i}$$
$$s.t. \; \sum_{i=1}^{N}a_{i}y_{i}=0$$
$$a_{i}\geq 0,i=1,2,...,N$$
## 线性支持向量机的软间隔最大化
引入松弛变量 $\xi _{i}\geq 0$，惩罚参数 $C>0$,原问题变为：
$$\underset{w,b}{min}\; \frac{1}{2}\left \| w \right \|^{2}+C\sum_{i=1}^{N}\xi _{i}$$
$$y_{i}\left ( w\cdot x_{i}+b\right )\geq 1-\xi _{i} ,\; i=1,2,...,N$$
$$\xi _{i}\geq 0,\; i=1,2,...,N$$
## 软间隔最大化的对偶算法
$$L(w,b,a,\mu ,\xi )=\frac{1}{2}\left \| w \right \|^{2}+C\sum_{i=1}^{N}\xi _{i}-\sum_{i=1}^{N}a_{i}(y_{i}\left ( w\cdot x_{i}+b)-1+\xi_{i}\right )-\sum_{i=1}^{N}\mu_{i}\xi_{i}$$
原问题的对偶问题为：
$$\underset{a}{max}\underset{w,b,\xi}{min}L(w,b,a,\mu ,\xi)$$
**学习算法**
>输入：训练数据集 $T=\{( x_{1},y_{1}),( x_{2},y_{2}),...,( x_{N},y_{N}) \},x_{i}\in R^n,y_{i}\in\{-1,1\},i=1,2,...,N$；  
>输出：分类决策函数  
>1. 选择惩罚参数 $C>0$,构造并解决凸二次规划问题
>$$\underset{a}{min}\; \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}a_{i}$$
>$$s.t. \; \sum_{i=1}^{N}a_{i}y_{i}=0$$
>$$0\leq a_{i}\leq C,i=1,2,...,N$$
>求得最优解 $a=(a_{1},a_{2},...,a_{N})^T$  
>2. 计算:  
>$$w=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$
>选择 $a$ 的一个合适的分量 $a_{j}$适合条件 $0<a_{j}<C$，计算：
>$$b=y_{j}-\sum_{i=1}^{N}y_{i}a_{i}(x_{i}\cdot x)$$
>3. 输出分类决策函数 
>$$f(x)=sign(w\cdot x+b)$$
## 核函数
$$K\left ( x,z \right )=\phi \left ( x \right )\cdot \phi \left ( z \right )$$
函数 $\phi \left ( x \right )$ 是从输入空间到希尔伯特空间的映射
对偶问题中的内积$x_{i}\cdot x_{j}$可以用核函数 $K\left ( x_{i},x_{j} \right )$ 来替代，在给定核函数的情况下学习是隐式地在特征空间进行的，不需要显示的知道映射函数。在实际应用中，一般直接选择核函数，其有效性通过实验验证。  
**对于一个具体的核函数（通常指正定核函数） $K\left ( x,z \right )$ ,检验其能否为正定核函数不容易，因为要对任意有限输入集验证$K$对应的$Gram$矩阵是否为半正定的。**
## 序列最小优化算法（SMO）