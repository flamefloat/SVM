# 支持向量机（SVM）
SVM是一种二分类模型，它的基本模型是定义在特征空间上的线性分类器，间隔最大化使其有别于感知机；通过核技巧可成为非线性分类器。支持向量机的学习算法是求解凸二次规划的最优解法。通过核函数学习非线性支持向量机，等价于隐式地在高维特征空空间学习线性支持向量机。
## SVM的学习算法
样本点 $\left ( x_{i},y_{i} \right )$ 被超平面 $\left ( w,b \right )$ 正确分类时，点 $x_{i}$ 到超平面的几何间隔为
$$\gamma _{i}=y_{i}\left ( \frac{w}{\left \| w \right \|}\cdot x_{i}+\frac{b}{\left \| w \right \|} \right )$$
函数间隔与几何间隔关系为
$$\gamma =\frac{\widehat{\gamma }}{\left \| w \right \|}$$
超平面参数 $\left ( w,b \right )$ 成比例变化，函数间隔也成比例变化，但几何间隔不变。

SVM问题为求解几何间隔最大化的超平面，即：
$$\underset{w,b}{max}\: \gamma $$
$$s.t.\; y_{i}\left ( \frac{w}{\left \| w \right \|}\cdot x_{i}+\frac{b}{\left \| w \right \|} \right )\geq \gamma ,\; i=1,2,...,N$$
等价于
$$\underset{w,b}{max}\: \frac{\widehat{\gamma }}{\left \| w \right \|} $$
$$s.t.\; y_{i}\left ( w\cdot x_{i}+b\right )\geq \widehat{\gamma } ,\; i=1,2,...,N$$
令 $\widehat{\gamma }=1$ 原问题等价与
$$\underset{w,b}{min}\; \frac{1}{2}\left \| w \right \|^{2}$$
$$y_{i}\left ( w\cdot x_{i}+b\right )-1\geq 0 ,\; i=1,2,...,N$$
**凸优化问题**
>$$\underset{w}{min}\; f(w)$$
>$$s.t.\; g_{i}(w)\leq 0,i=1,2,...,k$$
>$$h_{i}(w)= 0,i=1,2,...,l$$
>其中目标函数 $f(w)$ 和约束函数 $g_{i}(w)$ 都是 $R^{n}$ 上的连续可微的凸函数，约束函数 $h_{i}(w)$ 是 $R^{n}$ 上的的仿射函数。

求解问题得到超平面 $w\cdot x+b=0$，分类决策函数为：
$$f(x)=sign(w\cdot x+b)$$
## SVM学习的对偶算法
引入拉格朗日乘子 $a_{i}\geq 0,i=1,2,...,N$，定义拉格朗日函数：
$$L(w,b,a)=\frac{1}{2}\left \| w \right \|^{2}-\sum_{i=1}^{N}a_{i}y_{i}\left ( w\cdot x_{i}+b\right )+\sum_{i=1}^{N}a_{i}$$
原始问题的对偶问题是极大极小问题：
$$\underset{a}{max}\underset{w,b}{min}L(w,b,a)$$
将函数 $L(w,b,a)$ 分别对 $(w,b)$ 求偏导，并令其等于0，得：
$$w=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$
$$\sum_{i=1}^{N}a_{i}y_{i}=0$$
于是得到对偶优化问题：
$$\underset{a}{min}\; \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}a_{i}$$
$$s.t. \; \sum_{i=1}^{N}a_{i}y_{i}=0$$
$$a_{i}\geq 0,i=1,2,...,N$$
## 线性支持向量机的软间隔最大化
引入松弛变量 $\xi _{i}\geq 0$，惩罚参数 $C>0$,原问题变为：
$$\underset{w,b}{min}\; \frac{1}{2}\left \| w \right \|^{2}+C\sum_{i=1}^{N}\xi _{i}$$
$$y_{i}\left ( w\cdot x_{i}+b\right )\geq 1-\xi _{i} ,\; i=1,2,...,N$$
$$\xi _{i}\geq 0,\; i=1,2,...,N$$
## 软间隔最大化的对偶算法
$$L(w,b,a,\mu ,\xi )=\frac{1}{2}\left \| w \right \|^{2}+C\sum_{i=1}^{N}\xi _{i}-\sum_{i=1}^{N}a_{i}(y_{i}\left ( w\cdot x_{i}+b)-1+\xi_{i}\right )-\sum_{i=1}^{N}\mu_{i}\xi_{i}$$
原问题的对偶问题为：
$$\underset{a}{max}\underset{w,b,\xi}{min}L(w,b,a,\mu ,\xi)$$
**学习算法**
>输入：训练数据集 $T=\{( x_{1},y_{1}),( x_{2},y_{2}),...,( x_{N},y_{N}) \},x_{i}\in R^n,y_{i}\in\{-1,1\},i=1,2,...,N$；  
>输出：分类决策函数  
>1. 选择惩罚参数 $C>0$,构造并解决凸二次规划问题
>$$\underset{a}{min}\; \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}a_{i}$$
>$$s.t. \; \sum_{i=1}^{N}a_{i}y_{i}=0$$
>$$0\leq a_{i}\leq C,i=1,2,...,N$$
>求得最优解 $a=(a_{1},a_{2},...,a_{N})^T$  
>2. 计算:  
>$$w=\sum_{i=1}^{N}a_{i}y_{i}x_{i}$$
>选择 $a$ 的一个合适的分量 $a_{j}$适合条件 $0<a_{j}<C$，计算：
>$$b=y_{j}-\sum_{i=1}^{N}y_{i}a_{i}(x_{i}\cdot x)$$
>3. 输出分类决策函数 
>$$f(x)=sign(w\cdot x+b)$$
## 核函数
$$K\left ( x,z \right )=\phi \left ( x \right )\cdot \phi \left ( z \right )$$
函数 $\phi \left ( x \right )$ 是从输入空间到希尔伯特空间的映射
对偶问题中的内积$x_{i}\cdot x_{j}$可以用核函数 $K\left ( x_{i},x_{j} \right )$ 来替代，在给定核函数的情况下学习是隐式地在特征空间进行的，不需要显示的知道映射函数。在实际应用中，一般直接选择核函数，其有效性通过实验验证。  
**对于一个具体的核函数（通常指正定核函数） $K\left ( x,z \right )$ ,检验其能否为正定核函数不容易，因为要对任意有限输入集验证$K$对应的$Gram$矩阵是否为半正定的。**
## 序列最小优化算法（SMO）
凸二次规划存在全局最优解，但当训练样本很大时，很多算法变得非常低效，无法使用。  
SMO算法是一种启发式算法，基本思路为：如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。否则选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。子问题可用解析法求解，能大大提高计算速度。子问题有两个变量（**只有一个是自变量**），一个是违反KKT条件最严重的那个，另一个由约束条件确定。 

选择两个变量 $(a_{1},a_{2})$，固定其他变量 $a_{i}$,学习算法中凸二次规划问题的子问题为：
$$\underset{a_{1},a_{2}}{min}\; W(a_{1},a_{2})=\frac{1}{2}K_{11}a_{1}^{2}+\frac{1}{2}K_{22}a_{2}^{2}+y_{1}y_{2}K_{12}a_{1}a_{2}-\left ( a_{1}+a_{2} \right )+y_{1}a_{1}\sum_{i=3}^{N}y_{i}a_{i}K_{i1}+y_{2}a_{2}\sum_{i=3}^{N}y_{i}a_{i}K_{i2}$$
$$s.t.\; a_{1}y_{1}+a_{2}y_{2}=-\sum_{i=3}^{N}y_{i}a_{i}=\varsigma $$
$$0\leq a_{i}\leq C,i=1,2$$
其中 $K_{ij}=K(x_{i},x_{j}),\varsigma$为常数，目标函数中省略了不含 $(a_{1},a_{2})$的常数项。  
假设子问题的初始可行解为 $a_{1}^{old},a_{2}^{old}$,最优解为 $a_{1}^{new},a_{2}^{new}$,并且假设沿着约束方向未经剪辑时 $a_{2}$ 的最优解为 $a_{2}^{new,unc}$,于是有：
$$a_{2}^{new,unc}=a_{2}^{old}+\frac{y_{2}\left ( E_{1}-E_{2} \right )}{\eta }$$
其中：
$$E_{i}=g(x_{i})-y_{i},\;\;i=1,2$$
$$g(x)=\sum_{i=1}^{N}a_{i}y_{i}K\left ( x_{i},x \right )+b$$
$$\eta =K_{11}+K_{22}-2K_{12}$$
经剪辑后：
$$a_{2}^{new}=\left\{\begin{matrix}
H, & a_{2}^{new,unc}> H\\ 
a_{2}^{new,unc}, &L\leq a_{2}^{new,unc}\leq H  \\ 
L, &a_{2}^{new,unc}< L 
\end{matrix}\right.$$
若 $y_{1}\neq y_{2}$,则：
$$L=max(0,a_{2}^{old}-a_{1}^{old}),\; H=min(C,C+a_{2}^{old}-a_{1}^{old})$$
若 $y_{1}= y_{2}$,则：
$$L=max(0,a_{2}^{old}+a_{1}^{old}-C),\; H=min(C,a_{2}^{old}+a_{1}^{old})$$
有 $a_{2}^{new}$ 求得 $a_{1}^{new}$ 为：
$$a_{1}^{new}=a_{1}^{old}+y_{1}y_{2}(a_{2}^{old}-a_{2}^{new})$$
### 变量的选择方法
SMO算法在每个子问题中选择两个变量优化时，其中至少一个变量是违反KKT条件的。
1. 第一个变量选择  
>SMO称选择第一个变量为外层循环。**外层循环在训练样本中选取违反KKT条件最严重的样本点**，将其对应的变量作为第一个变量。检验是否满足KKT条件，即：  
>$$a_{i}=0 \Leftrightarrow y_{i}g(x_{i})\geq 1$$  
>$$0< a_{i}< C \Leftrightarrow y_{i}g(x_{i})=  1$$  
>$$a_{i}=  C \Leftrightarrow y_{i}g(x_{i})\leq   1$$  
>外层循环首先遍历 $0< a_{i}< C$的样本点支持向量，即在间隔边界上的支持向量点，若这些样本点都满足KKT条件，则遍历整个训练集。
2. 第二个变量选择
>SMO称选择第一个变量为内层循环。第二个变量选择标准是希望 $a_{2}$有足够大的变化。$a_{2}^{new}$ 依赖于 $\left | E_{1}-E_{2} \right |$,因为 $a_{1}$ 已定，$E_{1}$也确定了。如果 $E_{1}$为正，则选择最小的 $E_{i}$作为 $E_{2}$，反之则选择最大的 $E_{i}$作为 $E_{2}$。  
>在特殊情况下在内层循环找到的 $a_{2}$不能使目标函数有足够的下降，那么采用以下启发式规则继续选择 $a_{2}$。遍历在间隔边界上的支持向量点，依次将其对应的变量作为 $a_{2}$试用，直到目标函数有足够的下降。若找不到合适的 $a_{2}$，则遍历整个训练集，仍找不到合适的 $a_{2}$，则放弃第一个 $a_{1}$，再通过外层循环寻找 $a_{1}$
3. 计算阈值 $b$ 和差值 $E_{i}$  
>在每次完成变量优化后，要重新计算阈值 $b$ 和差值 $E_{i}$ ，当 $0< a_{1}^{new}< C$ 时，由KKT条件 $y_{i}g(x_{i})=  1$ 可知：  
>$$\sum_{i=1}^{N}a_{i}y_{i}K_{i1}+b=y_{1}$$
>由此得到：
>$$b_{1}^{new}=-E_{1}-y_{1}K_{11}\left ( a_{1}^{new}-a_{1}^{old} \right )-y_{2}K_{21}\left ( a_{2}^{new}-a_{2}^{old} \right )+b^{old}$$  
>同样，如果 $0< a_{2}^{new}< C$ 有：
>$$b_{2}^{new}=-E_{2}-y_{1}K_{12}\left ( a_{1}^{new}-a_{1}^{old} \right )-y_{2}K_{22}\left ( a_{2}^{new}-a_{2}^{old} \right )+b^{old}$$  
>如果 $a_{1}^{new},a_{2}^{new}$ 同时满足条件 $0< a_{i}^{new}< C,i=1,2$, 那么 $b_{1}^{new}=b_{2}^{new}$, 如果 $a_{1}^{new},a_{2}^{new}$ 是 0 或者 $C$, 那么 $b_{1}^{new}$ 和 $b_{2}^{new}$ 即他们之间的数都符合KKT条件，这是选取其中点作为 $b^{new}$  
>每次更新完两个变量后，还必须更新对应的 $E_{i}$ 值，并将其保存到列表中。$E_{i}$ 更新需要用到 $b^{new}$ 的值，以及所有支持向量对应的 $a_{j}$:  
>$$E_{i}^{new}=\sum_{s}y_{j}a_{j}K\left ( x_{i},x_{j} \right )+b^{new}-y_{i}$$
>$s$表示支持向量 $x_{j}$ 的集合。
### SMO算法
>输入：训练数据集 $T=\{( x_{1},y_{1}),( x_{2},y_{2}),...,( x_{N},y_{N}) \},x_{i}\in R^n,y_{i}\in\{-1,1\},i=1,2,...,N$, 精度 $\varepsilon$;  
>输出：近似解 $\widehat{a}$.  
>1. 取初值 $a^{(0)}=0$, 令 $k=0$;  
>2. 选取优化变量 $a_{1}^{(k)},a_{2}^{(k)}$,解析求解两个变量最优化问题，求得最优解 $a_{1}^{(k+1)},a_{2}^{(k+1)}$，更新 $a$ 为 $a^{(k+1)}$;  
>3. 若在精度 $\varepsilon$ 范围类满足停机条件：  
>$$\sum_{i=1}^{N}a_{i}y_{i}=0$$
>$$0\leq a_{i}\leq C,i=1,2,...,N$$
>$$y_{i}\cdot g\left ( x_{i} \right )=\left\{\begin{matrix}
\geq 1, &\{x_{i}|a_{i}=0\} \\ 
=1, &\: \: \: \: \: \: \: \: \{x_{i}|0< a_{i}< C\} \\ 
\leq 1,& \{x_{i}|a_{i}=C\}
\end{matrix}\right.$$  
>其中:  
>$$g(x_{i})=\sum_{j=1}^{N}a_{j}y_{j}K\left ( x_{j},x_{i} \right )+b$$  
>则转4；否则令 $k=k+1$，转2；  
>4. 取 $\widehat{a}=a^{(k+1)}$.